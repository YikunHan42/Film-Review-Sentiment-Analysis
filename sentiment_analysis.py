# -*- coding: utf-8 -*-
"""Sentiment Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12yCwZNx8eVBeOeJNJRl6IbrXnNgNJTNi

import libraries
"""

import pandas as pd # 数据处理库
import numpy as np # 数组/向量库
import matplotlib.pyplot as plt # 画图库
import seaborn as sns # 画图库
import nltk # NLP库
nltk.download('punkt') # nltk数据源

from nltk.tokenize import word_tokenize # 分词
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences # 向量补齐
from keras.models import Sequential # 一种序列化的数据格式
from keras.layers import Embedding, LSTM, Dense, Dropout #embedding其实就是个映射，文本数据域指向数值域；长短期记忆人工神经网络；全连接层；丢弃数据
from sklearn.preprocessing import LabelEncoder

import warnings # 避免warning
warnings.filterwarnings('ignore')
sns.set()

"""importing files"""

from google.colab import drive # google云盘导入
drive.mount('/content/drive')

"""importing data"""

imdb_train = pd.read_csv('/content/drive/My Drive/blog_train.csv') # 读入训练集
imdb_train.head()
imdb_test = pd.read_csv('/content/drive/My Drive/blog_test.csv') # 读入测试集

"""relatively balanced dataset"""

imdb_train.label.value_counts() # 平衡数据集

"""tokenizer demo"""

text = imdb_train['sentence'][0]
print(text) 
print(word_tokenize(text)) # 分词的演示

corpus = []
for text in imdb_train['sentence']:
    words = [word.lower() for word in word_tokenize(text)]
    corpus.append(words) # 变小写

num_words = len(corpus) # 获取长度
print(num_words)

imdb_train.shape # 向量 6377 * 2

X_train = imdb_train.sentence # 训练数据集输入
y_train = imdb_train.label # 训练数据集输出
X_test = imdb_test.sentence # 测试数据集输入
y_test = imdb_test.label # 测试数据集输出

type(X_train)

tokenizer = Tokenizer(num_words) # 创建分词对象
tokenizer.fit_on_texts(X_train) # 训练
type(X_train)

X_train = tokenizer.texts_to_sequences(X_train) # 文本转为序列
type(X_train)

X_train = pad_sequences(X_train, maxlen = 32, truncating = 'post', padding= 'post') # padding补齐到32个token
type(X_train)

X_train[0], len(X_train[0]) # 示例

X_test = tokenizer.texts_to_sequences(X_test) # 同上，测试集
X_test = pad_sequences(X_test, maxlen = 32, truncating ='post', padding= 'post')

X_test[10], len(X_test[0]) # 示例

print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape) # 向量形状 ，后面没写就是1

le = LabelEncoder() # 对分类型特征值进行编码，即对不连续的数值或文本进行编码
y_train = le.fit_transform(y_train)
y_test = le.transform(y_test)

"""Model"""

model = Sequential()

model.add(Embedding(input_dim = num_words, output_dim = 100,
          input_length = 32, trainable = True)) # 词向量映射，6377到100维
model.add(LSTM(100, dropout = 0.1, return_sequences = True)) # LSTM 输入100维，扔掉0.1的数据，防止过拟合
model.add(LSTM(100, dropout = 0.1)) # 同上
model.add(Dense(1, activation = 'sigmoid')) # 输出1维，类别变量，sigmoid作为激活函数

model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy']) # 交叉熵损失，adam优化器，获取accuracy

history = model.fit(X_train, y_train, epochs = 20, shuffle = True, batch_size = 64, validation_data = (X_test, y_test)) # 训练

plt.figure(figsize=(16,5)) # 图
epochs = range(1, len(history.history['accuracy'])+1)
plt.plot(epochs, history.history['accuracy'], 'b', label = 'Training Accuracy', color ='red')
plt.plot(epochs, history.history['val_accuracy'], 'b' ,label ='Validation Accuracy')
plt.legend()
plt.show()

validation_sentence = ['A film without love and beauty, but shining'] # 测试句子
validation_sentence_tokened = tokenizer.texts_to_sequences(validation_sentence) # 文本转化为序列
validation_sentence_padded = pad_sequences(validation_sentence_tokened, maxlen = 32, truncating = 'post', padding= 'post') # padding补齐

print(validation_sentence[0])
print("Probability of Positive:{}".format(model.predict(validation_sentence_padded)[0])) # 预测积极概率

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report

!pip install git+https://github.com/laxmimerit/preprocess_kgptalkie.git

import preprocess_kgptalkie as ps
import re 

def get_clean(x):
    x = str(x).lower().replace('\\', '').replace('_', ' ')
    x = ps.cont_exp(x)
    x = ps.remove_emails(x)
    x = ps.remove_urls(x)
    x = ps.remove_html_tags(x)
    x = ps.remove_rt(x)
    x = ps.remove_accented_chars(x)
    x = ps.remove_special_chars(x)
    x = re.sub("(.)\\1{2,}", "\\1", x)
    return x

X_train = imdb_train.sentence
y_train = imdb_train.label
X_test = imdb_test.sentence
y_test = imdb_test.label

X_train = X_train.apply(lambda x: get_clean(x))
X_test = X_test.apply(lambda x: get_clean(x))

tfidf = TfidfVectorizer(max_features = 675)

X_train = tfidf.fit_transform(X_train) 
X_test = tfidf.fit_transform(X_test)

X_train

clf = LinearSVC()
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
print(classification_report(y_test, y_pred))

x = 'A film without love and beauty, but shining'

x = get_clean(x)
vec = tfidf.transform([x])

clf.predict(vec)

douban = pd.read_csv('/content/drive/My Drive/chinese.csv')

douban['label'] = douban['label'].str.replace(r'[^0-9]', '')

douban.label.value_counts()

douban

douban = douban.drop(douban[(douban['label'] == '') | (douban['label'] == '9')| (douban['label'] == '1996')| (douban['label'] == '0117500')].index)
douban

douban.label.value_counts()

import jieba
douban['sentence'] = douban['sentence'].astype(str)
douban['sentence'] = douban.sentence.apply(lambda x: " ".join(jieba.cut(x)))
douban

import re
pattern = re.compile(r'[\u4e00-\u9fa5]+')
clean_data = []
for i, data in enumerate(douban['sentence']) :
  temp = re.compile(u'[\u4E00-\u9FA5|\s\w]').findall(data)
  datatemp1 = ''.join(temp)
  #datatemp2 = datatemp1.replace(' ', '')
  datatemp2 = datatemp1.lstrip()
  data = datatemp2.rstrip()
  douban['sentence'][i] = data
  
douban

#  data = re.findall(pattern, data)
#  data = ''.join(data)

#douban['comment'] = re.findall(pattern, douban['comment'])

douban = douban[0:50000]
douban

corpus = []
for text in douban['sentence']:
    words = [word.lower() for word in word_tokenize(text)]
    corpus.append(words) # 变小写

num_words = len(corpus) # 获取长度
print(num_words)

X_train = [str(a) for a in douban.sentence[0:40000].tolist()]

tokenizer.fit_on_texts(X_train)

print(tokenizer)

X_train = tokenizer.texts_to_sequences(X_train)

X_train[0]

X_train = pad_sequences(X_train, maxlen = 64, truncating = 'post', padding= 'post') # padding补齐到128个token

print(X_train[0])

X_test = [str(a) for a in douban.sentence[40000:50000].tolist()]
X_test = tokenizer.texts_to_sequences(X_test)
X_test = pad_sequences(X_test, maxlen = 64, truncating = 'post', padding= 'post') # padding补齐到128个token

print(X_train.shape, X_test.shape)

le = LabelEncoder() # 对分类型特征值进行编码，即对不连续的数值或文本进行编码
y_train = le.fit_transform(y_train)
y_test = le.transform(y_test)

model = Sequential()

model.add(Embedding(input_dim = num_words, output_dim = 200,
          input_length = 64, trainable = True)) # 
model.add(LSTM(200, dropout = 0.1, return_sequences = True)) # LSTM 输入100维，扔掉0.1的数据，防止过拟合
model.add(LSTM(200, dropout = 0.1)) # 同上
model.add(Dense(1, activation = 'sigmoid')) # 输出1维，类别变量，sigmoid作为激活函数

model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy']) # 交叉熵损失，adam优化器，获取accuracy

history = model.fit(X_train, y_train, epochs = 20, shuffle = True, batch_size = 200, validation_data = (X_test, y_test)) # 训练

plt.figure(figsize=(16,5)) # 图
epochs = range(1, len(history.history['accuracy'])+1)
plt.plot(epochs, history.history['accuracy'], 'b', label = 'Training Accuracy', color ='red')
plt.plot(epochs, history.history['val_accuracy'], 'b' ,label ='Validation Accuracy')
plt.legend()
plt.show()

tfidf = TfidfVectorizer(max_features = 10000)

X_train = douban.sentence[0:40000]
X_test = douban.sentence[40000:50000]
X_train = tfidf.fit_transform(X_train) 
X_test = tfidf.fit_transform(X_test)

clf = LinearSVC()
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
print(classification_report(y_test, y_pred))